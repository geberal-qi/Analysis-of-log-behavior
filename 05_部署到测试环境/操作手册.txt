1.部署nginx日志采集脚本
   在nginx服务器上安装flume，采集nginx/logs目录下的日志到
   hadoop集群的hdfs系统
   数据采集脚本：flume_nginx_log.conf
   启动flume-ng：
   bin/flume-ng agent -c ./conf -f ./conf/flume_nginx_log.conf -n a1 -Dflume.root.logger=INFO,console
2.将编写的spark程序打包上传到服务器
   执行spark-submit 执行脚本（需要设置成定时任务）
   使用azkban

	type=command
	command=/usr/local/spark-1.6.1-bin-hadoop2.6/bin/spark-submit \
	--class com.zhiyou100.LogApp \
	--master spark://hadoop01:7077 \
	--executor-memory 1G \
	--driver-memory  1G \
	--total-executor-cores 3 \
	/upload/log_app-1.0-SNAPSHOT.jar
   启动azkban web服务器:
   nohup  bin/azkaban-web-start.sh  1>/tmp/azstd.out  2>/tmp/azerr.out &

   执行服务器
   在执行服务器目录下执行启动命令
   bin/azkaban-executor-start.sh
   注:只能要执行服务器根目录运行
   启动完成后,在浏览器(建议使用谷歌浏览器)中输入https://hadoop01:8443 ,
   即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login



